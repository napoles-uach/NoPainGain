---
number: 5 <!-- leave as-is, maintainers will adjust -->
title: Comparing Bayesian Optimization Methods Across Multiple Hyperparameters Against Simulated "Human" Decision-making
topic: benchmark-task
team_leads: 
  - Kevin Shen (NobleAI)
  - Lawrence Wang (Erthos)
  - Mark Croxall (University of Toronto)
# Comment these lines to hide these elements
<!-- contributors:
 - Contributor 1 (Institution 1)
 - Contributor 2 (Institution 2) -->
 
<!-- github: AC-BO-Hackathon/<your-repo-name> -->

<!-- youtube_video: <your-video-id> -->
---

This project will focus on trying to simulate the decision-making of a human researcher using a Bayesian Optimization framework, then comparing the performance across different, improved hyperparameters. By exploring these differences, this project aims to understand the strengths and weaknesses of Bayesian Optimization relative to the decision-making of a researcher with access to the same data.

We will look at three hyperparameters that can be used to define the differences between a researcher and regular Bayesian Optimization: the number of features that can be processed, the degree of exploration vs exploitation, and the interpretability/complexity of the surrogate model.

Muckley, E. S., Saal, J. E., Meredig, B., Roper, C. S., Martin, J. H. (2023) [Interpretable models for extrapolation in scientific in machine learning](https://pubs.rsc.org/en/content/articlepdf/2023/dd/d3dd00082f). Digital Discovery, 2023, 2, 1425

